{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8632062-232f-4f85-9ee3-a0d228fb2346",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "698b61f4-cae0-4a06-b76c-64e8a0914583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up root directory to PATH\n",
    "import sys\n",
    "from pathlib import Path\n",
    "root_path = str(Path.cwd().parent)\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "# Import external libraries\n",
    "from typing import Tuple\n",
    "from scandeval import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoModelForTokenClassification, \n",
    "                          AutoConfig,\n",
    "                          AutoTokenizer,\n",
    "                          DataCollatorForTokenClassification,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          EarlyStoppingCallback)\n",
    "\n",
    "# Import local scripts\n",
    "from src import (ner_preprocess_data, ner_compute_metrics, NER_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d00bca-64b6-4e01-8b13-35aeb9ff7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer(df: pd.DataFrame) -> Tuple[Trainer, Dataset, Dataset]:\n",
    "    \n",
    "    # Convert dataframe to HuggingFace Dataset\n",
    "    dataset_dct = dict(doc=df.doc,\n",
    "                       tokens=df.tokens,\n",
    "                       orig_labels=df.ner_tags)\n",
    "    dataset = Dataset.from_dict(dataset_dct)\n",
    "    \n",
    "    # Tokenize and align labels\n",
    "    dataset = ner_preprocess_data(dataset, tokenizer)\n",
    "\n",
    "    # Set up training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='.',\n",
    "        evaluation_strategy='epoch',\n",
    "        logging_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        report_to='none',\n",
    "        save_total_limit=1,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=1000,\n",
    "        warmup_steps=len(dataset) * 0.9,\n",
    "        gradient_accumulation_steps=1,\n",
    "        load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "    # Split the dataset into a training and validation dataset\n",
    "    split = dataset.train_test_split(0.1, seed=4242)\n",
    "    \n",
    "    # Set up data collator for feeding the data into the model\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    \n",
    "    # Set up early stopping callback\n",
    "    early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "    \n",
    "    # Initialise the Trainer object\n",
    "    trainer = Trainer(model=model,\n",
    "                      args=training_args,\n",
    "                      train_dataset=split['train'],\n",
    "                      eval_dataset=split['test'],\n",
    "                      tokenizer=tokenizer,\n",
    "                      data_collator=data_collator,\n",
    "                      compute_metrics=ner_compute_metrics,\n",
    "                      callbacks=[early_stopping])\n",
    "    \n",
    "    # Return the trainer, the training dataset and the validation dataset\n",
    "    return trainer, split['train'], split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "939b173e-30ac-4883-b125-14eb4f9ae98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_id: str, id2label: list):\n",
    "    config = dict(num_labels=len(NER_LABELS),\n",
    "                  id2label=NER_LABELS,\n",
    "                  label2id={lbl:id for id, lbl in enumerate(NER_LABELS)})\n",
    "    config = AutoConfig.from_pretrained(trf, **config)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(trf)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(trf, config=config)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b398267-e68e-4c96-bf82-c29b78748a1e",
   "metadata": {},
   "source": [
    "##Â Load datasets and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bde73c-10cd-4b3c-a154-0a3fbe39d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['dane', 'norne-nb', 'norne-nn', 'suc3', 'wikiann-is', 'wikiann-fo']\n",
    "all_datasets = {name: pd.concat((load_dataset(name)[0], \n",
    "                                 load_dataset(name)[2]), axis=1) \n",
    "                for name in dataset_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6765d2ed-6305-4da7-a469-7f98f5a3f193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, tokenizer = load_model('NbAiLab/nb-bert-base', NER_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2692557-d9ff-4398-a4d5-f20ad7d988f1",
   "metadata": {},
   "source": [
    "## Concatenating all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa4e02-3394-44e3-bfe0-1a21834d39c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_concatenated = (pd.concat(all_datasets.values(), axis=0)\n",
    "                        .reset_index(drop=True))\n",
    "print(f'There are {len(fully_concatenated):,} documents in the dataset.')\n",
    "fully_concatenated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ca749-bba8-4d77-bc86-57af7923b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer, train, val = get_trainer(fully_concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff03453-c72b-4de3-8ec1-c88ebd97f021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70b4b1-08a9-478f-b864-3b48751ec3c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0a9b8d-227e-499d-a54f-5954b42add8e",
   "metadata": {},
   "source": [
    "## Ensuring equal language contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714cdab-b001-4783-a8f1-7c539be361f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = min(len(df) for df in dataset_dict.values())\n",
    "subsampled = (pd.concat([df.sample(min_length) \n",
    "                        for df in datasets.values()], axis=0)\n",
    "                .reset_index(drop=True))\n",
    "print(f'There are {len(subsampled):,} documents in the dataset.')\n",
    "subsampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a18c280-a451-489f-9a52-26aabddc3ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer, train, val = get_trainer(subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951568e-3b00-4d7a-b711-b295d1b92a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()subsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ea2dcb-da46-49f3-8265-69fd68f4ae8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
